{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=\"<YOUR-API-KEY>\", environment=\"<YOUR-ENVIRONMENT>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the tokenizer and model (OpenAI's GPT-2 model is used as an example)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('covid19_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].str.replace(r'http\\S+', '', flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace(r'[^\\w\\s]', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_usernames_links(tweet):\n",
    "        # tweet = re.sub('@[^\\s]+','',str(tweet))\n",
    "        # tweet = re.sub('#[^\\s]+','',str(tweet))\n",
    "        tweet = re.sub('http[^\\s]+','',str(tweet))\n",
    "        return tweet\n",
    "df['text'] = df['text'].apply(remove_usernames_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to encode text into vector\n",
    "def encode_text(text):\n",
    "    global i\n",
    "    # inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # if len(inputs) > 512:\n",
    "    #     print(text)\n",
    "    #     inputs = inputs[:512]\n",
    "    # outputs = model(**inputs)\n",
    "    # return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    # print(text)\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    # print(inputs)\n",
    "    # print(len(inputs[0]))\n",
    "    if len(inputs[0]) > 512:\n",
    "        # print(text)\n",
    "        inputs = inputs[0][:512]\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs)[0]\n",
    "    # print(embeddings)\n",
    "    # if len(embeddings[0]) > 512:\n",
    "    #     # print(embeddings[0], len(embeddings[0]))\n",
    "    #     embeddings[0] = embeddings[0][:512]\n",
    "    # print(len(embeddings[0]))\n",
    "    # print(i,end=\"\")\n",
    "    # i+=1\n",
    "    return embeddings.mean(dim=1).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=1\n",
    "# Assuming 'df' is your DataFrame and it has a column 'text' which contains the text documents\n",
    "vectors = df['text'].apply(encode_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectors.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectors[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['abstract'][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=[]\n",
    "vs = []\n",
    "for i in range(len(vectors)):\n",
    "    if len(vectors[i].tolist()) == 768:\n",
    "        vectors[i][0] = vectors[i][0].astype('float64')\n",
    "        vs.append(vectors[i].tolist())\n",
    "        ls.append({\"id\":str(i),\"values\":vectors[i].tolist()})\n",
    "    # ls.append(vectors[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ls:\n",
    "    if len(i['values']) != 768:\n",
    "        print(i['id'], len(i['values']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a Pinecone index\n",
    "if \"semantic-search\" in pinecone.list_indexes():\n",
    "    pinecone.delete_index(\"semantic-search\")\n",
    "pinecone.create_index(name=\"semantic-search\",dimension=768,metric='cosine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.Index(\"semantic-search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Upsert vectors into Pinecone\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(i+batch_size, len(df))\n",
    "    # create IDs batch\n",
    "    ids = [str(x) for x in range(i, i_end)]\n",
    "    # create metadata batch\n",
    "    metadatas = [{'text': df['text'].iloc[i]} for i in range(i,i_end)]\n",
    "    print(metadatas)\n",
    "    # create embeddings\n",
    "    xc = vs[i:i_end]\n",
    "    # create records list for upsert\n",
    "    records = zip(ids, xc, metadatas)\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=records)\n",
    "\n",
    "# check number of records in the index\n",
    "index.describe_index_stats()\n",
    "\n",
    "# index.upsert(vectors=ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for semantic search\n",
    "def search(query):\n",
    "    query_vector = encode_text(query)\n",
    "    query_vector = query_vector.astype('float64')\n",
    "    query_vector = query_vector.tolist()\n",
    "    \n",
    "    results = index.query(vector=query_vector, top_k=10)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Search\n",
    "query = \"eat healthy food during this pandamic period\"\n",
    "res = search(query)\n",
    "print(res)\n",
    "for j in res['matches']:\n",
    "    # print(df.iloc[int(j['id'])]['title'],\"\\n\")\n",
    "    print(df.iloc[int(j['id'])]['text'])\n",
    "    print(\"Similarity: \",j['score'],\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('covid19_tweets_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
